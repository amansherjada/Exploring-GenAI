{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:02:07.287168Z","iopub.execute_input":"2025-01-28T08:02:07.287512Z","iopub.status.idle":"2025-01-28T08:02:13.117542Z","shell.execute_reply.started":"2025-01-28T08:02:07.287478Z","shell.execute_reply":"2025-01-28T08:02:13.115689Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# **Tokenization**\n\n**Tokenization** is the process of breaking down text into smaller units, called **tokens**, which can be words, subwords, characters, or even bytes, depending on the tokenization technique used. In the context of Large Language Models (LLMs), tokenization is a crucial preprocessing step that enables the model to understand and process textual input effectively.\n\n---\n\n## **Why is Tokenization Important in LLMs?**\n\n1. **Efficient Text Representation:**\n   - Computers process numbers, not raw text. Tokenization converts text into a numerical format (tokens) that the LLM can understand and work with.\n   - These tokens are mapped to unique integer IDs via a vocabulary, which the model uses for computations.\n\n2. **Handling Language Complexity:**\n   - Tokenization helps in breaking down complex, unstructured language into manageable parts.\n   - Subword tokenization techniques (e.g., Byte Pair Encoding, WordPiece) allow LLMs to handle rare or unknown words effectively by splitting them into smaller, meaningful components.\n\n3. **Reducing Vocabulary Size:**\n   - Instead of using an entire dictionary of words, subword-based tokenization reduces the vocabulary size by creating tokens for frequent subword units, making the model memory-efficient and easier\n  \n## Examples of **tokenization**:\n\n---\n\n### **1. Word-Based Tokenization**\n- Text: *\"Tokenization is fun!\"*  \n- Tokens: `[\"Tokenization\", \"is\", \"fun\", \"!\"]`\n\n---\n\n### **2. Character-Based Tokenization**\n- Text: *\"Fun!\"*  \n- Tokens: `[\"F\", \"u\", \"n\", \"!\"]`\n\n---\n\n### **3. Subword-Based Tokenization (e.g., Byte Pair Encoding or WordPiece)**\n- Text: *\"Unbelievable!\"*  \n- Tokens: `[\"Un\", \"believable\", \"!\"]`  \n  *(The word is split into smaller subword units.)*\n\n---\n\n### **4. Byte-Level Tokenization (Used in GPT Models)**\n- Text: *\"Hello ðŸ˜Š\"*  \n- Tokens: `[\"Hello\", \" \", \"ðŸ˜Š\"]`  \n  *(Emojis and spaces are treated as separate tokens.)*\n\n---\n\n### **5. Sentence Tokenization**\n- Text: *\"I love coding. It's amazing!\"*  \n- Tokens: `[\"I love coding.\", \"It's amazing!\"]`  \n  *(Breaks text into sentences.)*","metadata":{}},{"cell_type":"markdown","source":"### [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSequenceClassification)\n\n### [AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n\n### [Model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n\nmodel_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n\nmy_model = AutoModelForSequenceClassification.from_pretrained(model_name)\nmy_tokenizer = AutoTokenizer.from_pretrained(model_name)\n\nclassifier = pipeline(\"sentiment-analysis\", model= my_model, tokenizer = my_tokenizer)\nresult = classifier(\"Not happy with the world\")\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:23:41.259500Z","iopub.execute_input":"2025-01-28T08:23:41.259905Z","iopub.status.idle":"2025-01-28T08:23:42.517021Z","shell.execute_reply.started":"2025-01-28T08:23:41.259879Z","shell.execute_reply":"2025-01-28T08:23:42.515844Z"}},"outputs":[{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"},{"name":"stdout","text":"[{'label': '2 stars', 'score': 0.5226433873176575}]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"result2 = classifier(\"I am happy with the world\")\nprint(result2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:24:17.100470Z","iopub.execute_input":"2025-01-28T08:24:17.100853Z","iopub.status.idle":"2025-01-28T08:24:17.176079Z","shell.execute_reply.started":"2025-01-28T08:24:17.100824Z","shell.execute_reply":"2025-01-28T08:24:17.174802Z"}},"outputs":[{"name":"stdout","text":"[{'label': '5 stars', 'score': 0.6275884509086609}]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Text Tokenization","metadata":{}},{"cell_type":"code","source":"# Example Text\ntext = \"I am happy with the world\"\n\n# Tokenize the Text\ntokens = my_tokenizer.tokenize(text)\nprint(\"Token\", tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:28:32.070694Z","iopub.execute_input":"2025-01-28T08:28:32.071127Z","iopub.status.idle":"2025-01-28T08:28:32.077081Z","shell.execute_reply.started":"2025-01-28T08:28:32.071101Z","shell.execute_reply":"2025-01-28T08:28:32.075992Z"}},"outputs":[{"name":"stdout","text":"Token ['i', 'am', 'happy', 'with', 'the', 'world']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Convert tokens to input IDs\ninput_ids = my_tokenizer.convert_tokens_to_ids(tokens)\nprint(input_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:32:53.843811Z","iopub.execute_input":"2025-01-28T08:32:53.844189Z","iopub.status.idle":"2025-01-28T08:32:53.849239Z","shell.execute_reply.started":"2025-01-28T08:32:53.844162Z","shell.execute_reply":"2025-01-28T08:32:53.848135Z"}},"outputs":[{"name":"stdout","text":"[151, 10345, 19308, 10171, 10103, 10228]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Encode the text(tokenization + converting to input_ids)\n\nencoded_input = my_tokenizer(text)\nprint(\"Encoded Text:\", encoded_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:36:54.169120Z","iopub.execute_input":"2025-01-28T08:36:54.169536Z","iopub.status.idle":"2025-01-28T08:36:54.175799Z","shell.execute_reply.started":"2025-01-28T08:36:54.169505Z","shell.execute_reply":"2025-01-28T08:36:54.174692Z"}},"outputs":[{"name":"stdout","text":"Encoded Text: {'input_ids': [101, 151, 10345, 19308, 10171, 10103, 10228, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Decode the text\n\ndecoded_input = my_tokenizer.decode(encoded_input[\"input_ids\"])\nprint(\"Decoded Text:\", decoded_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-28T08:37:55.053943Z","iopub.execute_input":"2025-01-28T08:37:55.054375Z","iopub.status.idle":"2025-01-28T08:37:55.060060Z","shell.execute_reply.started":"2025-01-28T08:37:55.054339Z","shell.execute_reply":"2025-01-28T08:37:55.058616Z"}},"outputs":[{"name":"stdout","text":"Decoded Text: [CLS] i am happy with the world [SEP]\n","output_type":"stream"}],"execution_count":16}]}