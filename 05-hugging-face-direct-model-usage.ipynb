{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Hugging Face Direct Model Usage with Examples**\nUsing Hugging Face models directly provides more control over the entire process, including tokenization, model inference, and decoding. Below is a step-by-step guide.","metadata":{}},{"cell_type":"code","source":"pip -q install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T12:04:13.156448Z","iopub.execute_input":"2025-01-22T12:04:13.156797Z","iopub.status.idle":"2025-01-22T12:04:18.545372Z","shell.execute_reply.started":"2025-01-22T12:04:13.156770Z","shell.execute_reply":"2025-01-22T12:04:18.543833Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T12:04:18.546791Z","iopub.execute_input":"2025-01-22T12:04:18.547136Z","iopub.status.idle":"2025-01-22T12:04:18.854087Z","shell.execute_reply.started":"2025-01-22T12:04:18.547096Z","shell.execute_reply":"2025-01-22T12:04:18.853238Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T12:04:18.855598Z","iopub.execute_input":"2025-01-22T12:04:18.855912Z","iopub.status.idle":"2025-01-22T12:04:20.219649Z","shell.execute_reply.started":"2025-01-22T12:04:18.855880Z","shell.execute_reply":"2025-01-22T12:04:20.218529Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# **1. Import Necessary Libraries and Load a Pre-trained Model and Tokenizer**\n\nUse AutoTokenizer and AutoModelForCausalLM to load the tokenizer and model. Replace \"meta-llama/Llama-3.2-1B\" with the desired model name.","metadata":{}},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T12:04:20.220880Z","iopub.execute_input":"2025-01-22T12:04:20.221177Z","iopub.status.idle":"2025-01-22T12:05:53.160447Z","shell.execute_reply.started":"2025-01-22T12:04:20.221151Z","shell.execute_reply":"2025-01-22T12:05:53.159286Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ed9af68e9ce4a8fab7683b341e9b7f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84331deba92b43e79981f96c6aa62fba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4c416e793464f01af87b76294239bd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"473f72caf20f453c9a8fc15bcdc38250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"747beb9831964a59a5cc0a4f231e311c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a3a17299ec2456090419eae0df69c24"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# **2. Tokenize the Input Text**\nTokenization converts text into numerical representations (tokens) that the model can understand.","metadata":{}},{"cell_type":"code","source":"input_text = \"High in the halls of the kings who are gone\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")  # \"pt\" for PyTorch tensors\nprint(inputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T12:05:53.161664Z","iopub.execute_input":"2025-01-22T12:05:53.162337Z","iopub.status.idle":"2025-01-22T12:05:53.177677Z","shell.execute_reply.started":"2025-01-22T12:05:53.162291Z","shell.execute_reply":"2025-01-22T12:05:53.176751Z"}},"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[128000,  12243,    304,    279,  52473,    315,    279,  45619,    889,\n            527,   8208]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"The `return_tensors` parameter in Hugging Face's tokenizer specifies the type of tensors that should be returned after tokenizing the input text. The **options** for `return_tensors` determine the framework to which the tensors are compatible. Here's a breakdown:\n\n---\n\n## **1. What is `return_tensors=\"pt\"`?**\n- `pt` stands for **PyTorch**, a popular deep learning framework.\n- When you use `return_tensors=\"pt\"`, the tokenizer returns a PyTorch tensor that is directly compatible with PyTorch-based models.\n\nExample:\n```python\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntokens = tokenizer(\"Hello, Hugging Face!\", return_tensors=\"pt\")\nprint(tokens)\n```\n\nOutput:\n```python\n{'input_ids': tensor([[  101,  7592,  1010, 17662,  2227,   999,   102]]), \n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n```\n\nHere:\n- `input_ids`: Encoded tokens of the input sentence.\n- `attention_mask`: Binary mask indicating which tokens should be attended to (1) and which should be ignored (0).\n\n---\n\n## **2. Other Options for `return_tensors`**\n\n### **a. `return_tensors=\"tf\"`**\n- Returns a **TensorFlow** tensor.\n- Used when working with TensorFlow/Keras models.\n- Example:\n  ```python\n  tokens = tokenizer(\"Hello, Hugging Face!\", return_tensors=\"tf\")\n  print(tokens)\n  ```\n  Output:\n  ```python\n  {'input_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[  101,  7592,  1010, 17662,  2227,   999,   102]])>, \n   'attention_mask': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1]])>}\n  ```\n\n---\n\n### **b. `return_tensors=\"np\"`**\n- Returns a **NumPy array**, which is not tied to a specific deep learning framework.\n- Useful for preprocessing, lightweight testing, or when you don’t need a specific framework.\n- Example:\n  ```python\n  tokens = tokenizer(\"Hello, Hugging Face!\", return_tensors=\"np\")\n  print(tokens)\n  ```\n  Output:\n  ```python\n  {'input_ids': array([[  101,  7592,  1010, 17662,  2227,   999,   102]]), \n   'attention_mask': array([[1, 1, 1, 1, 1, 1, 1]])}\n  ```\n\n---\n\n## **3. When to Use Each Option**\n\n| Option          | Use Case                                                                                       |\n|------------------|-----------------------------------------------------------------------------------------------|\n| `return_tensors=\"pt\"` | When working with **PyTorch** models.                                                     |\n| `return_tensors=\"tf\"` | When working with **TensorFlow/Keras** models.                                            |\n| `return_tensors=\"np\"` | When you want a general-purpose **NumPy** array for preprocessing or lightweight analysis.|\n\n---\n\n## **4. What Happens If You Don’t Use `return_tensors`?**\nIf you don’t specify `return_tensors`, the tokenizer returns **Python lists** by default. While this format is readable and straightforward, it's not optimized for computation and cannot be directly passed to models.\n\nExample:\n```python\ntokens = tokenizer(\"Hello, Hugging Face!\")\nprint(tokens)\n```\n\nOutput:\n```python\n{'input_ids': [101, 7592, 1010, 17662, 2227, 999, 102], \n 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n```\n\nYou would need to manually convert these lists into tensors compatible with your framework.\n\n---\n\n### **Summary**\n- `return_tensors` is essential for converting tokenized data into tensors compatible with deep learning frameworks.\n- Options:\n  - `\"pt\"`: PyTorch tensors.\n  - `\"tf\"`: TensorFlow tensors.\n  - `\"np\"`: NumPy arrays.\n- Choose the option based on the framework you're using.","metadata":{}},{"cell_type":"markdown","source":"# **3. Generate Output Using the Model**\n\nUse the model's generate method to generate text based on the input tokens.","metadata":{}},{"cell_type":"code","source":"outputs = model.generate(\n    inputs[\"input_ids\"],  # Input tokens\n    max_length=50,        # Maximum length of the output sequence\n    num_return_sequences=1,  # Number of sequences to generate\n    do_sample=True,         # Enable sampling for creative outputs\n    \n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T12:05:53.178681Z","iopub.execute_input":"2025-01-22T12:05:53.179035Z","iopub.status.idle":"2025-01-22T12:06:03.294746Z","shell.execute_reply.started":"2025-01-22T12:05:53.179002Z","shell.execute_reply":"2025-01-22T12:06:03.293642Z"}},"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# **4. Decode the Generated Tokens**\nThe model outputs tokens, which you need to decode back into readable text.","metadata":{}},{"cell_type":"code","source":"generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T12:06:03.295797Z","iopub.execute_input":"2025-01-22T12:06:03.296113Z","iopub.status.idle":"2025-01-22T12:06:03.302497Z","shell.execute_reply.started":"2025-01-22T12:06:03.296087Z","shell.execute_reply":"2025-01-22T12:06:03.301317Z"}},"outputs":[{"name":"stdout","text":"High in the halls of the kings who are gone\nThey know the way to the great hall\nThe way to the great hall\nWhere the kings who are gone are laid\nThe way to the great hall\nWhere the kings who are gone\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# **Advanced Options**\n## **1. Modify Generation Parameters**\nYou can control the output by adjusting parameters like:\n- `temperature`: Controls randomness (lower = less random, higher = more creative).\n- `top_k`: Limits token selection to the top K most probable tokens.\n- `top_p`: Enables nucleus sampling (tokens with cumulative probability ≤ `top_p`).\n\nExample:\n```python\noutputs = model.generate(\n    inputs[\"input_ids\"],\n    max_length=50,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.9,\n    num_return_sequences=3\n)\n```\n\n## **2. Batch Processing**\nYou can process multiple inputs simultaneously for efficiency.\n```python\ninput_texts = [\"Hello, how are you?\", \"Once upon a time,\"]\ninputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\noutputs = model.generate(inputs[\"input_ids\"], max_length=50)\n```\n\n## **3. Save and Load Models Locally**\nIf you want to save the model locally:\n```python\nmodel.save_pretrained(\"./local_model\")\ntokenizer.save_pretrained(\"./local_model\")\n```\nTo load it later:\n```python\ntokenizer = AutoTokenizer.from_pretrained(\"./local_model\")\nmodel = AutoModelForCausalLM.from_pretrained(\"./local_model\")\n```\n\n---\n\n## **Documentation and Tutorials**\n\n1. **Hugging Face Transformers Documentation:**\n   - [AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto)\n   - [AutoModelForCausalLM](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM)\n   - [Text generation strategies](https://huggingface.co/docs/transformers/main/en/generation_strategies#text-generation-strategies)\n\n2. **Hugging Face Tutorials:**\n   - [Getting Started with Transformers](https://huggingface.co/docs/transformers/main/en/index)\n   - [Generation with Transformers](https://huggingface.co/blog/how-to-generate)\n\n3. **Model Repository:**\n   - [Meta-Llama Models](https://huggingface.co/meta-llama)\n\n4. **Hugging Face Forums:**\n   - [Community Discussions](https://discuss.huggingface.co/)","metadata":{}}]}