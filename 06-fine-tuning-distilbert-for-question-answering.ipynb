{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Brief Overview:**  \nThis notebook demonstrates the workflow for fine-tuning a BERT-based model for question-answering tasks. It covers the following steps:  \n1. Loading and exploring the dataset.  \n2. Initializing the BERT model and tokenizer.  \n3. Preprocessing the dataset to prepare inputs for the model.  \n4. Configuring lightweight training parameters for efficient fine-tuning.  \n5. Training the model on the dataset and saving the fine-tuned version.  \n6. Loading the trained model and predicting answers for given questions and contexts.  \n7. Providing a practical example of using the fine-tuned model for real-world question-answering.","metadata":{}},{"cell_type":"code","source":"pip install -q transformers datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:03:36.682221Z","iopub.execute_input":"2025-01-22T13:03:36.682580Z","iopub.status.idle":"2025-01-22T13:03:40.764789Z","shell.execute_reply.started":"2025-01-22T13:03:36.682550Z","shell.execute_reply":"2025-01-22T13:03:40.763807Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:04:01.664965Z","iopub.execute_input":"2025-01-22T13:04:01.665255Z","iopub.status.idle":"2025-01-22T13:04:01.756108Z","shell.execute_reply.started":"2025-01-22T13:04:01.665231Z","shell.execute_reply":"2025-01-22T13:04:01.755511Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:04:36.620171Z","iopub.execute_input":"2025-01-22T13:04:36.620487Z","iopub.status.idle":"2025-01-22T13:04:37.320115Z","shell.execute_reply.started":"2025-01-22T13:04:36.620427Z","shell.execute_reply":"2025-01-22T13:04:37.319483Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# **Import Necessary Libraries**","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    DistilBertTokenizerFast,  # This is a tokenizer specifically designed for DistilBERT. \n                              # It converts input text into token IDs (numerical representations) \n                              # that the model can understand. The \"Fast\" version is optimized for speed \n                              # and efficiency using the Hugging Face `tokenizers` library.\n\n    DistilBertForQuestionAnswering,  # This is a pre-trained DistilBERT model architecture fine-tuned \n                                      # specifically for Question Answering tasks. It is capable of \n                                      # taking a context paragraph and a question as input and predicting \n                                      # the start and end positions of the answer within the context.\n\n    TrainingArguments,  # This is a helper class provided by Hugging Face Transformers. \n                        # It allows you to define hyperparameters and settings for model training, \n                        # such as batch size, learning rate, number of epochs, and more.\n\n    Trainer,  # This is a high-level API for training and evaluating models. \n              # It abstracts away many lower-level details of the training loop, \n              # such as managing batches, gradients, and evaluation metrics.\n\n    DataCollatorWithPadding  # This is a utility class used during training to dynamically pad input sequences \n                             # in a batch to the same length. Padding ensures that sequences of different lengths \n                             # can be processed together in batches, without wasting computation on excessive padding.\n)\nfrom datasets import load_dataset  # This function is part of the Hugging Face Datasets library.\n                                   # It allows you to load pre-built or custom datasets from Hugging Face's \n                                   # dataset hub or your local files. It supports efficient dataset loading, \n                                   # preprocessing, and manipulation for training and evaluation.\n\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:35:08.012671Z","iopub.execute_input":"2025-01-22T14:35:08.012972Z","iopub.status.idle":"2025-01-22T14:35:08.017302Z","shell.execute_reply.started":"2025-01-22T14:35:08.012950Z","shell.execute_reply":"2025-01-22T14:35:08.016408Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# **Load [Dataset](https://huggingface.co/datasets/rajpurkar/squad)**","metadata":{}},{"cell_type":"code","source":"# Load a tiny dataset subset\ndataset = load_dataset(\"squad\", split=\"train[:200]\") # Only 200 Examples\neval_dataset = load_dataset(\"squad\", split=\"validation[:40]\") # 40 validation examples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:24:30.391416Z","iopub.execute_input":"2025-01-22T13:24:30.391769Z","iopub.status.idle":"2025-01-22T13:24:33.920476Z","shell.execute_reply.started":"2025-01-22T13:24:30.391744Z","shell.execute_reply":"2025-01-22T13:24:33.919868Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a902d49c57c444ab41947f1791a93f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aee8b5b4bee147ab8bd67f3924bdd4a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2632a783fa345fe97e8e8caf82046cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"358ff239b69542b189f1e1c06764396c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a6c6acb5b924eb884befb6caca128ea"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# **Load [Model](https://huggingface.co/distilbert/distilbert-base-uncased) and Tokenizer**","metadata":{}},{"cell_type":"code","source":"model_name = \"distilbert/distilbert-base-uncased\" #Smaller version of the BERT base model\n\ntokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\nmodel = DistilBertForQuestionAnswering.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:29:22.779501Z","iopub.execute_input":"2025-01-22T13:29:22.779819Z","iopub.status.idle":"2025-01-22T13:29:25.075800Z","shell.execute_reply.started":"2025-01-22T13:29:22.779799Z","shell.execute_reply":"2025-01-22T13:29:25.075177Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab07742a32e9416492106ebd73979bc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a7916a49b4747f7ac0447e40322c42f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f8f7c6068c5420c8dbfed995ea99bff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ebd707229d14b9dbd18f5954d2abfd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92b4571e894b48c484c06d78c135f07d"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# **Preprocessing**","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    # Strip leading/trailing whitespace from questions and contexts\n    questions = [q.strip() for q in examples[\"question\"]]  # Clean each question in the dataset\n    contexts = [c.strip() for c in examples[\"context\"]]  # Clean each context in the dataset\n\n    # Tokenize the questions and contexts\n    inputs = tokenizer(\n        questions,  # List of questions to tokenize\n        contexts,   # Corresponding list of contexts\n        max_length=256,  # Maximum token length for input sequences\n        truncation=\"only_second\",  # Truncate the context (second input) if it exceeds max_length\n        stride=128,  # Overlap between truncated segments for better context handling\n        return_offsets_mapping=True,  # Include the mapping of tokens to original character positions\n        padding=\"max_length\"  # Pad sequences to max_length for uniformity\n    )\n\n    # Lists to store the start and end positions of the answers in tokenized inputs\n    start_positions = []\n    end_positions = []\n\n    # Iterate over each example in the dataset\n    for i, offset in enumerate(inputs[\"offset_mapping\"]):  # 'offset_mapping' links tokens to their original text spans\n        answer = examples[\"answers\"][i]  # Get the answer for the current example\n        start_char = answer[\"answer_start\"][0]  # Character index of the answer's start\n        end_char = start_char + len(answer[\"text\"][0])  # Character index of the answer's end\n\n        # Initialize start and end token indices\n        start_token = 0\n        end_token = 0\n\n        # Iterate over the token offsets to find the start and end tokens\n        for idx, (start, end) in enumerate(offset):\n            if start <= start_char and end >= start_char:  # Check if the token contains the start_char\n                start_token = idx\n            if start <= end_char and end >= end_char:  # Check if the token contains the end_char\n                end_token = idx\n                break  # End token found; exit loop\n\n        # Append the identified token indices to the respective lists\n        start_positions.append(start_token)\n        end_positions.append(end_token)\n\n    # Add the calculated start and end positions to the tokenized inputs\n    inputs[\"start_positions\"] = start_positions\n    inputs[\"end_positions\"] = end_positions\n\n    # Return the preprocessed inputs\n    return inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T13:58:57.792633Z","iopub.execute_input":"2025-01-22T13:58:57.792951Z","iopub.status.idle":"2025-01-22T13:58:57.799382Z","shell.execute_reply.started":"2025-01-22T13:58:57.792927Z","shell.execute_reply":"2025-01-22T13:58:57.798533Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Process Datasets\n\ntokenized_dataset = dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=32,\n    remove_columns=dataset.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:02:27.588895Z","iopub.execute_input":"2025-01-22T14:02:27.589221Z","iopub.status.idle":"2025-01-22T14:02:27.756021Z","shell.execute_reply.started":"2025-01-22T14:02:27.589192Z","shell.execute_reply":"2025-01-22T14:02:27.755085Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19b5595ad78b4b5fa492742d4caf2190"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"tokenized_dataset = eval_dataset.map(\n    preprocess_function,\n    batched=True,\n    batch_size=32,\n    remove_columns=dataset.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:03:46.453422Z","iopub.execute_input":"2025-01-22T14:03:46.453738Z","iopub.status.idle":"2025-01-22T14:03:46.512402Z","shell.execute_reply.started":"2025-01-22T14:03:46.453715Z","shell.execute_reply":"2025-01-22T14:03:46.511501Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8694367df2f045de8a9ac3c9eb0f5bd2"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"pip -q install transformers[torch]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:09:46.065810Z","iopub.execute_input":"2025-01-22T14:09:46.066144Z","iopub.status.idle":"2025-01-22T14:09:49.394971Z","shell.execute_reply.started":"2025-01-22T14:09:46.066115Z","shell.execute_reply":"2025-01-22T14:09:49.394037Z"}},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# **Configuring Training Parameters for Fast and Lightweight Model Training**","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./qa-results\",  # Directory where model checkpoints and outputs will be saved\n    num_train_epochs=1,  # Number of times the entire training dataset will be passed through the model\n    per_device_train_batch_size=4,  # Batch size for training per device (e.g., per GPU or CPU)\n    per_device_eval_batch_size=4,  # Batch size for evaluation per device\n    learning_rate=5e-5,  # Learning rate used by the optimizer for adjusting model weights\n    weight_decay=0.01,  # Regularization technique to prevent overfitting by penalizing large weights\n    logging_steps=10,  # Log training progress every 10 steps\n    eval_strategy=\"no\",  # No evaluation will be performed during training\n    save_strategy=\"no\",  # No model checkpoints will be saved during training\n    use_cpu=True,  # Forces the training to use the CPU instead of a GPU (even if one is available)\n    report_to=\"none\"  # Disables logging to external platforms like WandB or TensorBoard\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:20:12.292324Z","iopub.execute_input":"2025-01-22T14:20:12.292665Z","iopub.status.idle":"2025-01-22T14:20:12.297249Z","shell.execute_reply.started":"2025-01-22T14:20:12.292638Z","shell.execute_reply":"2025-01-22T14:20:12.296512Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# **Initalize and Train**","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model= model,\n    args = training_args,\n    train_dataset=tokenized_dataset,\n    data_collator= DataCollatorWithPadding(tokenizer)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:22:28.292348Z","iopub.execute_input":"2025-01-22T14:22:28.292676Z","iopub.status.idle":"2025-01-22T14:22:28.317908Z","shell.execute_reply.started":"2025-01-22T14:22:28.292651Z","shell.execute_reply":"2025-01-22T14:22:28.317018Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# **Train and Save**","metadata":{}},{"cell_type":"code","source":"trainer.train()\nmodel.save_pretrained(\"./qa-model\")\ntokenizer.save_pretrained(\"./qa-model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:27:19.258145Z","iopub.execute_input":"2025-01-22T14:27:19.258428Z","iopub.status.idle":"2025-01-22T14:27:40.581561Z","shell.execute_reply.started":"2025-01-22T14:27:19.258407Z","shell.execute_reply":"2025-01-22T14:27:40.580717Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 00:17, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>4.960900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"('./qa-model/tokenizer_config.json',\n './qa-model/special_tokens_map.json',\n './qa-model/vocab.txt',\n './qa-model/added_tokens.json',\n './qa-model/tokenizer.json')"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"# **Loading a Pretrained DistilBERT Question-Answering Model and Tokenizer**","metadata":{}},{"cell_type":"code","source":"def load_qa_model(model_path=\"/kaggle/working/qa-model\"):\n    #Load model and tokenizer from the saved directory\n    tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n    model = DistilBertForQuestionAnswering.from_pretrained(model_path)\n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:33:45.891313Z","iopub.execute_input":"2025-01-22T14:33:45.891684Z","iopub.status.idle":"2025-01-22T14:33:45.895633Z","shell.execute_reply.started":"2025-01-22T14:33:45.891656Z","shell.execute_reply":"2025-01-22T14:33:45.894747Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# **Function to Predict Answers from a Given Question and Context Using DistilBERT**","metadata":{}},{"cell_type":"code","source":"def answer_question(question, context, model, tokenizer):\n    # Tokenize input question and context\n    inputs = tokenizer(\n        question,  # The question to be answered\n        context,  # The context or passage containing the answer\n        return_tensors=\"pt\",  # Return PyTorch tensors for model compatibility\n        max_length=256,  # Limit the tokenized input length to 256 tokens\n        truncation=\"only_second\",  # Truncate the context (second input) if too long\n        padding=True  # Add padding to make inputs uniform in length\n    )\n    \n    # Get model predictions without computing gradients\n    with torch.no_grad():  # Disable gradient computation for faster inference\n        outputs = model(**inputs)  # Pass tokenized inputs through the model\n    \n    # Find the most likely start and end positions for the answer\n    answer_start = torch.argmax(outputs.start_logits)  # Index of the highest start score\n    answer_end = torch.argmax(outputs.end_logits)  # Index of the highest end score\n    \n    # Convert token positions to string (reconstruct the answer text)\n    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])  # Map token IDs back to tokens\n    answer = tokenizer.convert_tokens_to_string(tokens[answer_start:answer_end + 1])  # Convert token sequence to a string\n    \n    return answer  # Return the predicted answer text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:33:46.987213Z","iopub.execute_input":"2025-01-22T14:33:46.987549Z","iopub.status.idle":"2025-01-22T14:33:46.992759Z","shell.execute_reply.started":"2025-01-22T14:33:46.987521Z","shell.execute_reply":"2025-01-22T14:33:46.991783Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# **Example Usage of Question-Answering Model to Extract Answers from Context**","metadata":{}},{"cell_type":"code","source":"model, tokenizer = load_qa_model()\n\n# Example context and question\ncontext = \"\"\"\nPython is a high-level programming language created by Guido van Rossum and released in 1991. \nPython's design emphasizes code readability with its notable use of significant whitespace. \nIts language constructs and object-oriented approach aim to help programmers write clear, logical code.\n\"\"\"\n\nquestion = \"Who created Python?\"\n\n# Get answer\nanswer = answer_question(question, context, model, tokenizer)\nprint(f\"\\nQuestion: {question}\")\nprint(f\"Answer: {answer}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T14:35:12.791470Z","iopub.execute_input":"2025-01-22T14:35:12.791745Z","iopub.status.idle":"2025-01-22T14:35:12.943570Z","shell.execute_reply.started":"2025-01-22T14:35:12.791727Z","shell.execute_reply":"2025-01-22T14:35:12.942675Z"}},"outputs":[{"name":"stdout","text":"\nQuestion: Who created Python?\nAnswer: ? [SEP] python is a high - level programming language created by guido van rossum and released in 1991\n","output_type":"stream"}],"execution_count":20}]}